{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKMRVEvPbQVU"
   },
   "source": [
    "# **Collaborative Graph Learning with Auxiliary Text for Temporal Event Prediction in Healthcare**\n",
    "\n",
    "Chang Lu, Chandan K. Reddy, Prithwish Chakraborty, Samantha Kleinberg, Yue Ning\n",
    "\n",
    "[IJCAI 2021](https://www.ijcai.org/proceedings/2021/0486.pdf)\n",
    "\n",
    "Model Parameters in Original Paper:\n",
    "\n",
    "1.   Learning Rate: 0.001\n",
    "2.   Number of Epochs: 200\n",
    "3.   Batch Size: 32\n",
    "4.   $d_c$: 32\n",
    "5.   $d_p$: 16\n",
    "6.   GRU Hidden Layer Dimension: 200\n",
    "7.   Graph Layer Number $L$: 2\n",
    "8.   $d_{c}^{(1)}$: 64\n",
    "9.   $d_{p}^{(1)}$: 32\n",
    "10.  $d_{c}^{(2)}$: 128\n",
    "11.  Attention Dimension: 32\n",
    "\n",
    "PyTorch Implementation by [Leisheng Yu](https://github.com/ThunderbornSakana) (leisheng.yu@alumni.emory.edu)\n",
    "\n",
    "Code adapted from https://github.com/LuChang-CS/CGL\n",
    "\n",
    "Text part not included (for fair comparison)\n",
    "\n",
    "# **Diagnosis Prediction -- Multi-label Binary Prediction Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK7g548ahypH"
   },
   "source": [
    "## **Package Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F_qhsBEFa_mz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from collections import OrderedDict\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwWd0EZEh7yu"
   },
   "source": [
    "## **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8BpgES3ih-LK"
   },
   "outputs": [],
   "source": [
    "# Non-Binary Format Combo for CGL\n",
    "train_codes_x = np.load(\"../MIMIC3_data/Nonbinary_Data_Format/train_codes_x.npy\")\n",
    "test_codes_x = np.load(\"../MIMIC3_data/Nonbinary_Data_Format/test_codes_x.npy\")\n",
    "train_codes_y = np.load('../MIMIC3_data/Nonbinary_Data_Format/train_codes_y.npy')\n",
    "train_visit_lens = np.load('../MIMIC3_data/Nonbinary_Data_Format/train_visit_lens.npy')\n",
    "test_codes_y = np.load('../MIMIC3_data/Nonbinary_Data_Format/test_codes_y.npy')\n",
    "test_visit_lens = np.load('../MIMIC3_data/Nonbinary_Data_Format/test_visit_lens.npy')\n",
    "\n",
    "code_levels = np.load('../MIMIC3_data/code_related/code_levels.npy')\n",
    "patient_code_adj = np.load('../MIMIC3_data/code_related/patient_code_adj.npy')\n",
    "code_code_adj = np.load('../MIMIC3_data/code_related/code_code_adj.npy')\n",
    "with open('../MIMIC3_data/code_related/code_map.pkl', 'rb') as f13:\n",
    "    code_map = pickle.load(f13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jbyJHa6N0Lf1"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Tj7scYz00TbL"
   },
   "outputs": [],
   "source": [
    "train_lens = torch.from_numpy(train_visit_lens).to(device)\n",
    "train_x = torch.from_numpy(train_codes_x).to(device)\n",
    "train_y = torch.from_numpy(train_codes_y).to(device)\n",
    "test_lens = torch.from_numpy(test_visit_lens).to(device)\n",
    "test_x = torch.from_numpy(test_codes_x).to(device)\n",
    "test_y = torch.from_numpy(test_codes_y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mAGDEqVY0XOi"
   },
   "outputs": [],
   "source": [
    "class MyData(data.Dataset):\n",
    "    def __init__(self, data_seq, data_label, data_len):\n",
    "        self.data_seq = data_seq\n",
    "        self.data_label = data_label\n",
    "        self.data_len = data_len\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data_seq)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_seq[idx], self.data_label[idx], self.data_len[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_6-6KU40vhm"
   },
   "source": [
    "## **Model Starts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VifoLSlxmdY-"
   },
   "outputs": [],
   "source": [
    "def sequence_mask(lengths, maxlen):\n",
    "    mask = ~(torch.ones((len(lengths), maxlen)).to(device).cumsum(dim=1).t() > lengths).t()\n",
    "    return mask\n",
    "\n",
    "def masked_softmax(inputs, mask):\n",
    "    inputs = inputs - torch.max(inputs, dim=-1, keepdim=True)[0]\n",
    "    exp = torch.exp(inputs) * mask\n",
    "    result = exp / (torch.sum(exp, dim=-1, keepdim=True) + 1e-12)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DNG1Hmt80x4S"
   },
   "outputs": [],
   "source": [
    "# 3.2 The Proposed Model - CGL\n",
    "# Hierarchical Embedding for Medical Codes\n",
    "class HierarchicalEmbedding(nn.Module):\n",
    "    def __init__(self, code_levels, code_num_in_levels, code_dims):\n",
    "        super(HierarchicalEmbedding, self).__init__()\n",
    "        self.level_num = len(code_num_in_levels)\n",
    "        self.code_levels = code_levels\n",
    "        self.level_embeddings = nn.ModuleList([nn.Embedding(code_num, code_dim) for level, (code_num, code_dim) in enumerate(zip(code_num_in_levels, code_dims))])\n",
    "\n",
    "    def forward(self, input=None):\n",
    "        embeddings = [self.level_embeddings[level](self.code_levels[:, level] - 1) for level in range(self.level_num)]\n",
    "        embeddings = torch.cat(embeddings, dim=1)\n",
    "        return embeddings#return: (code_num, embedding_size*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CdU4Uux18YOF"
   },
   "outputs": [],
   "source": [
    "# Graph Representation + Collaborative Graph Learning\n",
    "class GraphConvBlock(torch.nn.Module):\n",
    "  def __init__(self, node_type, input_dim, output_dim, adj):\n",
    "    super(GraphConvBlock, self).__init__()\n",
    "    self.node_type = node_type\n",
    "    self.adj = adj\n",
    "    self.dense = torch.nn.Linear(input_dim, output_dim)\n",
    "    self.bn = torch.nn.BatchNorm1d(output_dim)\n",
    "    self.activation = torch.nn.ReLU()\n",
    "\n",
    "  def forward(self, embedding, embedding_neighbor, weight_decay=None):\n",
    "    output = embedding + torch.matmul(self.adj, embedding_neighbor)\n",
    "    if self.node_type == 'code':\n",
    "        assert weight_decay is not None\n",
    "        output += torch.matmul(weight_decay, embedding)\n",
    "    output = self.dense(output)\n",
    "    output = self.bn(output)\n",
    "    output = self.activation(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JUpQwVxFUqxJ"
   },
   "outputs": [],
   "source": [
    "class GraphLearning(torch.nn.Module):\n",
    "  def __init__(self, patient_dim, code_dim, patient_code_adj, code_code_adj, patient_hidden_dims, code_hidden_dims):\n",
    "    super(GraphLearning, self).__init__()\n",
    "    # Setup\n",
    "    self.patient_code_adj = patient_code_adj\n",
    "    self.code_patient_adj = patient_code_adj.t()\n",
    "    self.code_code_adj = code_code_adj\n",
    "    self.code_num = code_code_adj.shape[0]\n",
    "    self.sigma = torch.nn.Sigmoid()\n",
    "    # Parameters\n",
    "    # For Ontology Weights\n",
    "    self.miu = torch.nn.Parameter(torch.randn(self.code_num), requires_grad=True)\n",
    "    self.theta = torch.nn.Parameter(torch.randn(self.code_num), requires_grad=True)\n",
    "    # For L=1 Graph Layer\n",
    "    self.c2p_dense_1 = torch.nn.Linear(code_dim, patient_dim)\n",
    "    self.p2c_dense_1 = torch.nn.Linear(patient_dim, code_dim)\n",
    "    self.patient_block_1 = GraphConvBlock('patient', patient_dim, patient_hidden_dims[0], self.patient_code_adj)\n",
    "    self.code_block_1 = GraphConvBlock('code', code_dim, code_hidden_dims[0], self.code_patient_adj)\n",
    "    # For L=2 Graph Layer\n",
    "    self.p2c_dense_2 = torch.nn.Linear(patient_hidden_dims[0], code_hidden_dims[0])\n",
    "    self.code_block_2 = GraphConvBlock('code', code_hidden_dims[0], code_hidden_dims[1], self.code_patient_adj)\n",
    "\n",
    "  def forward(self, patient_embeddings, code_embeddings):\n",
    "    ontology_weight_matrix = torch.sigmoid(self.miu * self.code_code_adj + self.theta)\n",
    "    # L = 1\n",
    "    code_embeddings_p = self.c2p_dense_1(code_embeddings)\n",
    "    patient_embeddings_new = self.patient_block_1(patient_embeddings, code_embeddings_p)\n",
    "    patient_embeddings_c = self.p2c_dense_1(patient_embeddings)\n",
    "    code_embeddings = self.code_block_1(code_embeddings, patient_embeddings_c, ontology_weight_matrix)\n",
    "    patient_embeddings = patient_embeddings_new\n",
    "    # L = 2\n",
    "    patient_embeddings_c = self.p2c_dense_2(patient_embeddings)\n",
    "    code_embeddings = self.code_block_2(code_embeddings, patient_embeddings_c, ontology_weight_matrix)\n",
    "    return code_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Z-jfNCLobwMG"
   },
   "outputs": [],
   "source": [
    "# Temporal Learning for Visits\n",
    "class VisitEmbedding(torch.nn.Module):\n",
    "  def __init__(self, max_seq_len):\n",
    "    super(VisitEmbedding, self).__init__()\n",
    "    self.max_seq_len = max_seq_len\n",
    "\n",
    "  def forward(self, code_embeddings, visit_codes, visit_lens):\n",
    "    # visit_codes: (batch_size, max_seq_len, max_code_num_in_a_visit)\n",
    "    visit_codes = visit_codes - 1\n",
    "    visit_codes_mask = (visit_codes == -1)\n",
    "    visit_codes[visit_codes_mask] = 0\n",
    "    visit_codes_mask = (1 - visit_codes_mask.float())\n",
    "    visit_codes_num = visit_codes_mask.float().sum(dim=-1).unsqueeze(-1)\n",
    "    visit_codes_embedding = code_embeddings[visit_codes] # (batch_size, max_seq_len, max_code_num_in_a_visit, code_dim)\n",
    "    visit_codes_mask = visit_codes_mask.unsqueeze(-1)\n",
    "    visit_codes_embedding *= visit_codes_mask  # (batch_size, max_seq_len, max_code_num_in_a_visit, code_dim)\n",
    "    assert not torch.isnan(visit_codes_embedding).any()\n",
    "    visit_codes_num[visit_codes_num == 0] = 1\n",
    "    visits_embeddings = torch.sum(visit_codes_embedding, dim=-2) / visit_codes_num # (batch_size, max_seq_len, code_dim)\n",
    "    assert not torch.isnan(visits_embeddings).any()\n",
    "    visit_mask = sequence_mask(visit_lens, self.max_seq_len)  # (batch_size, max_seq_len, 1)\n",
    "    visits_embeddings *= visit_mask.unsqueeze(-1)  # (batch_size, max_seq_len, code_dim)\n",
    "    return visits_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "s_LknJqzqdJg"
   },
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "  def __init__(self, input_dim, attention_dim):\n",
    "    super(Attention, self).__init__()\n",
    "    self.attention_dim = attention_dim\n",
    "    self.u_omega = torch.nn.Parameter(torch.randn(attention_dim), requires_grad=True)\n",
    "    self.w_omega = torch.nn.Parameter(torch.randn(input_dim, attention_dim), requires_grad=True)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    # x: (batch_size, max_seq_len, rnn_dim[-1] / hidden_size)\n",
    "    t = torch.matmul(x, self.w_omega)\n",
    "    vu = torch.matmul(t, self.u_omega).view(x.shape[:-1])  # (batch_size, max_seq_len)\n",
    "    vu *= mask\n",
    "    alphas = masked_softmax(vu, mask)\n",
    "    output = torch.sum(x * alphas.unsqueeze(-1), dim=-2)  # (batch_size, rnn_dim[-1] / hidden_size)\n",
    "    return output, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ml384RUHqhFW"
   },
   "outputs": [],
   "source": [
    "class TemporalEmbedding(torch.nn.Module):\n",
    "  def __init__(self, input_dim, rnn_dims, attention_dim, max_seq_len):\n",
    "    super(TemporalEmbedding, self).__init__()\n",
    "    self.rnn_layers = torch.nn.GRU(input_dim, rnn_dims[-1])\n",
    "    self.attention = Attention(rnn_dims[-1], attention_dim)\n",
    "    self.max_seq_len = max_seq_len\n",
    "\n",
    "  def forward(self, embeddings, lens):\n",
    "    seq_mask = sequence_mask(lens, self.max_seq_len)\n",
    "    outputs, hn = self.rnn_layers(embeddings)\n",
    "    outputs = outputs * seq_mask.unsqueeze(-1)  # (batch_size, max_seq_len, rnn_dim[-1])\n",
    "    outputs, alphas = self.attention(outputs, seq_mask)  # (batch_size, rnn_dim[-1])\n",
    "    return outputs, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "V2R7-zWq04nz"
   },
   "outputs": [],
   "source": [
    "class CGL(torch.nn.Module):\n",
    "  def __init__(self, code_map, code_levels, patient_code_adj, code_code_adj, num_train_sample, max_admission_num):\n",
    "    super(CGL, self).__init__()\n",
    "    # Hierarchical Embedding for Medical Codes\n",
    "    code_num_in_levels = (np.max(code_levels, axis=0)).tolist()\n",
    "    code_levels = torch.from_numpy(code_levels).to(device)\n",
    "    code_dims = [32] * code_levels.shape[1]\n",
    "    self.hier_embed_layer = HierarchicalEmbedding(code_levels, code_num_in_levels, code_dims)\n",
    "    # Initialize Patient Embeddings\n",
    "    self.user_emb = torch.nn.Embedding(num_train_sample, 16)\n",
    "    # Collaborative Graph Learning\n",
    "    patient_code_adj = torch.from_numpy(patient_code_adj).to(device).to(torch.float32)\n",
    "    code_code_adj = torch.from_numpy(code_code_adj).to(device)\n",
    "    code_code_adj = (code_code_adj > 0).float()\n",
    "    self.graph_convolution_layer = GraphLearning(\n",
    "        patient_dim=16,\n",
    "        code_dim=sum(code_dims),\n",
    "        patient_code_adj=patient_code_adj,\n",
    "        code_code_adj=code_code_adj,\n",
    "        patient_hidden_dims=[32],\n",
    "        code_hidden_dims=[64, 128])\n",
    "    # Temporal Learning for Visits\n",
    "    self.visit_embedding_layer = VisitEmbedding(max_admission_num)\n",
    "    self.visit_temporal_embedding_layer = TemporalEmbedding(128, [200], 32, max_admission_num)\n",
    "    # Output FC Layer\n",
    "    self.output_layer = torch.nn.Linear(200, len(code_map))\n",
    "    self.softmax = torch.nn.Softmax()\n",
    "\n",
    "  def forward(self, visit_codes, visit_lens):\n",
    "    # Get Hierarchical Embedding for Medical Codes\n",
    "    code_embeddings = self.hier_embed_layer(None)\n",
    "    # Get Patient Embeddings\n",
    "    patient_embeddings = self.user_emb.weight\n",
    "    # Collaborative Graph Learning\n",
    "    assert not torch.isnan(code_embeddings).any()\n",
    "    code_embeddings = self.graph_convolution_layer(patient_embeddings, code_embeddings)\n",
    "    # Temporal Learning for Visits\n",
    "    visits_embeddings = self.visit_embedding_layer(code_embeddings, visit_codes, visit_lens)\n",
    "    assert not torch.isnan(visits_embeddings).any()\n",
    "    visit_output, alpha_visit = self.visit_temporal_embedding_layer(visits_embeddings, visit_lens)\n",
    "    assert not torch.isnan(visit_output).any()\n",
    "    # Output\n",
    "    output = self.softmax(self.output_layer(visit_output))\n",
    "    return output, code_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ou6-LZxDt9eU"
   },
   "source": [
    "## **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QinZYbJ6t_gy"
   },
   "outputs": [],
   "source": [
    "model = CGL(code_map, code_levels, patient_code_adj, code_code_adj, len(train_x), torch.Tensor.size(train_x)[1])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "b2BthMo_UFyW"
   },
   "outputs": [],
   "source": [
    "# This is for diagnosis prediction\n",
    "def evaluate_model(pred, label, k1, k2, k3, k4, k5, k6):\n",
    "  pred2_k1 = torch.zeros_like(pred[0])\n",
    "  pred3_k1 = []\n",
    "  pred2_k2 = torch.zeros_like(pred[0])\n",
    "  pred3_k2 = []\n",
    "  pred2_k3 = torch.zeros_like(pred[0])\n",
    "  pred3_k3 = []\n",
    "  pred2_k4 = torch.zeros_like(pred[0])\n",
    "  pred3_k4 = []\n",
    "  pred2_k5 = torch.zeros_like(pred[0])\n",
    "  pred3_k5 = []\n",
    "  pred2_k6 = torch.zeros_like(pred[0])\n",
    "  pred3_k6 = []\n",
    "  # above is for recall and precision\n",
    "  true3 = [] # this is for label\n",
    "  pred4 = [] # this is for ndcg\n",
    "  for i in range(len(pred)):\n",
    "    pred2_k1[torch.topk(pred[i], k1).indices] = 1\n",
    "    pred3_k1.append(pred2_k1.cpu().detach().tolist())\n",
    "    pred2_k2[torch.topk(pred[i], k2).indices] = 1\n",
    "    pred3_k2.append(pred2_k2.cpu().detach().tolist())\n",
    "    pred2_k3[torch.topk(pred[i], k3).indices] = 1\n",
    "    pred3_k3.append(pred2_k3.cpu().detach().tolist())\n",
    "    pred2_k4[torch.topk(pred[i], k4).indices] = 1\n",
    "    pred3_k4.append(pred2_k4.cpu().detach().tolist())\n",
    "    pred2_k5[torch.topk(pred[i], k5).indices] = 1\n",
    "    pred3_k5.append(pred2_k5.cpu().detach().tolist())\n",
    "    pred2_k6[torch.topk(pred[i], k6).indices] = 1\n",
    "    pred3_k6.append(pred2_k6.cpu().detach().tolist())\n",
    "    pred4.append(pred[i].cpu().detach().tolist())\n",
    "    true3.append(label[i].cpu().detach().tolist())\n",
    "  \n",
    "  metric_p_1 = precision_score(true3, pred3_k1, average='samples')\n",
    "  metric_p_2 = precision_score(true3, pred3_k2, average='samples')\n",
    "  metric_p_3 = precision_score(true3, pred3_k3, average='samples')\n",
    "  metric_p_4 = precision_score(true3, pred3_k4, average='samples')\n",
    "  metric_p_5 = precision_score(true3, pred3_k5, average='samples')\n",
    "  metric_p_6 = precision_score(true3, pred3_k6, average='samples')\n",
    "  \n",
    "  metric_r_1 = recall_score(true3, pred3_k1, average='samples')\n",
    "  metric_r_2 = recall_score(true3, pred3_k2, average='samples')\n",
    "  metric_r_3 = recall_score(true3, pred3_k3, average='samples')\n",
    "  metric_r_4 = recall_score(true3, pred3_k4, average='samples')\n",
    "  metric_r_5 = recall_score(true3, pred3_k5, average='samples')\n",
    "  metric_r_6 = recall_score(true3, pred3_k6, average='samples')\n",
    "  \n",
    "  metric_n_1 = ndcg_score(true3, pred4, k=k1)\n",
    "  metric_n_2 = ndcg_score(true3, pred4, k=k2)\n",
    "  metric_n_3 = ndcg_score(true3, pred4, k=k3)\n",
    "  metric_n_4 = ndcg_score(true3, pred4, k=k4)\n",
    "  metric_n_5 = ndcg_score(true3, pred4, k=k5)\n",
    "  metric_n_6 = ndcg_score(true3, pred4, k=k6)\n",
    "  return metric_p_1, metric_r_1, metric_n_1, metric_p_2, metric_r_2, metric_n_2, metric_p_3, metric_r_3, metric_n_3, metric_p_4, metric_r_4, metric_n_4, metric_p_5, metric_r_5, metric_n_5, metric_p_6, metric_r_6, metric_n_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9ZoByWsjured"
   },
   "outputs": [],
   "source": [
    "# Initialize evaluation record lists\n",
    "metric_p1_list = []\n",
    "metric_p2_list = []\n",
    "metric_p3_list = []\n",
    "metric_p4_list = []\n",
    "metric_p5_list = []\n",
    "metric_p6_list = []\n",
    "metric_r1_list = []\n",
    "metric_r2_list = []\n",
    "metric_r3_list = []\n",
    "metric_r4_list = []\n",
    "metric_r5_list = []\n",
    "metric_r6_list = []\n",
    "metric_n1_list = []\n",
    "metric_n2_list = []\n",
    "metric_n3_list = []\n",
    "metric_n4_list = []\n",
    "metric_n5_list = []\n",
    "metric_n6_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5g2iF54uugv"
   },
   "outputs": [],
   "source": [
    "# Training mode\n",
    "model.train()\n",
    "# Loss and optimizer (learning rate)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# Initialize data loader for training\n",
    "training_data = MyData(train_x, train_y, train_lens)\n",
    "train_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "total_step = len(train_loader)\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "  for i, (patients, labels, seq_lengths) in enumerate(train_loader):\n",
    "    patients = patients.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Forward pass\n",
    "    outputs, learned_code = model(patients, seq_lengths)\n",
    "    loss = criterion(outputs, labels.to(torch.float32))\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Tracking\n",
    "    if (i+1) % 100 == 0:\n",
    "      print('Epoch: [{}/{}], Step: [{}/{}], Loss: {}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "  if (epoch+1) % 1 == 0:\n",
    "    model.eval()\n",
    "    # Initialize data loader for testing\n",
    "    test_data = MyData(test_x, test_y, test_lens)\n",
    "    test_loader = DataLoader(test_data, batch_size=len(test_x), shuffle=True)\n",
    "    # Testing\n",
    "    for (patients, labels, seq_lengths) in test_loader:\n",
    "        visits_embeddings = model.visit_embedding_layer(learned_code, patients, seq_lengths)\n",
    "        assert not torch.isnan(visits_embeddings).any()\n",
    "        visit_output, alpha_visit = model.visit_temporal_embedding_layer(visits_embeddings, seq_lengths)\n",
    "        assert not torch.isnan(visit_output).any()\n",
    "        pred = model.softmax(model.output_layer(visit_output))\n",
    "        # Subject to Change! @k for evaluation\n",
    "        metric_p1, metric_r1, metric_n1, metric_p2, metric_r2, metric_n2, metric_p3, metric_r3, metric_n3, metric_p4, metric_r4, metric_n4, metric_p5, metric_r5, metric_n5, metric_p6, metric_r6, metric_n6, = evaluate_model(pred, labels, 5, 10, 15, 20, 25, 30)\n",
    "        ###############################\n",
    "        metric_p1_list.append(metric_p1)\n",
    "        metric_p2_list.append(metric_p2)\n",
    "        metric_p3_list.append(metric_p3)\n",
    "        metric_p4_list.append(metric_p4)\n",
    "        metric_p5_list.append(metric_p5)\n",
    "        metric_p6_list.append(metric_p6)\n",
    "        ###############################\n",
    "        metric_r1_list.append(metric_r1)\n",
    "        metric_r2_list.append(metric_r2)\n",
    "        metric_r3_list.append(metric_r3)\n",
    "        metric_r4_list.append(metric_r4)\n",
    "        metric_r5_list.append(metric_r5)\n",
    "        metric_r6_list.append(metric_r6)\n",
    "        ###############################\n",
    "        metric_n1_list.append(metric_n1)\n",
    "        metric_n2_list.append(metric_n2)\n",
    "        metric_n3_list.append(metric_n3)\n",
    "        metric_n4_list.append(metric_n4)\n",
    "        metric_n5_list.append(metric_n5)\n",
    "        metric_n6_list.append(metric_n6)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mz1fEQpcdhCO"
   },
   "source": [
    "# **Mortality Prediction -- Binary Prediction Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45mjrirr14Cp"
   },
   "source": [
    "## **Package Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iaEz7PT814Cz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from collections import OrderedDict\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bl8TjLw17Al"
   },
   "source": [
    "## **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W2qqD0bs17Al"
   },
   "outputs": [],
   "source": [
    "# Non-Binary Format Combo for CGL\n",
    "train_codes_x = np.load(\"../MIMIC3_data/Nonbinary_Data_Format/train_codes_x.npy\")\n",
    "test_codes_x = np.load(\"../MIMIC3_data/Nonbinary_Data_Format/test_codes_x.npy\")\n",
    "train_mort = np.load('../MIMIC3_data/Nonbinary_Data_Format/train_mort.npy')\n",
    "train_visit_lens = np.load('../MIMIC3_data/Nonbinary_Data_Format/train_visit_lens.npy')\n",
    "test_mort = np.load('../MIMIC3_data/Nonbinary_Data_Format/test_mort.npy')\n",
    "test_visit_lens = np.load('../MIMIC3_data/Nonbinary_Data_Format/test_visit_lens.npy')\n",
    "\n",
    "code_levels = np.load('../MIMIC3_data/code_related/code_levels.npy')\n",
    "patient_code_adj = np.load('../MIMIC3_data/code_related/patient_code_adj.npy')\n",
    "code_code_adj = np.load('../MIMIC3_data/code_related/code_code_adj.npy')\n",
    "with open('../MIMIC3_data/code_related/code_map.pkl', 'rb') as f13:\n",
    "  code_map = pickle.load(f13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YovR2dMc17Am"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j2LpGc4417Am"
   },
   "outputs": [],
   "source": [
    "train_lens = torch.from_numpy(train_visit_lens).to(device)\n",
    "train_x = torch.from_numpy(train_codes_x).to(device)\n",
    "train_y = torch.from_numpy(train_mort).to(device)\n",
    "test_lens = torch.from_numpy(test_visit_lens).to(device)\n",
    "test_x = torch.from_numpy(test_codes_x).to(device)\n",
    "test_y = torch.from_numpy(test_mort).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yPcyIw_e17Am"
   },
   "outputs": [],
   "source": [
    "class MyData(data.Dataset):\n",
    "    def __init__(self, data_seq, data_label, data_len):\n",
    "        self.data_seq = data_seq\n",
    "        self.data_label = data_label\n",
    "        self.data_len = data_len\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data_seq)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_seq[idx], self.data_label[idx], self.data_len[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2X5lHwg2VCJ"
   },
   "source": [
    "## **Model Starts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "R9VLZQ652VCT"
   },
   "outputs": [],
   "source": [
    "def sequence_mask(lengths, maxlen):\n",
    "    mask = ~(torch.ones((len(lengths), maxlen)).to(device).cumsum(dim=1).t() > lengths).t()\n",
    "    return mask\n",
    "\n",
    "def masked_softmax(inputs, mask):\n",
    "    inputs = inputs - torch.max(inputs, dim=-1, keepdim=True)[0]\n",
    "    exp = torch.exp(inputs) * mask\n",
    "    result = exp / (torch.sum(exp, dim=-1, keepdim=True) + 1e-12)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JxebCgDT2VCT"
   },
   "outputs": [],
   "source": [
    "# 3.2 The Proposed Model - CGL\n",
    "# Hierarchical Embedding for Medical Codes\n",
    "class HierarchicalEmbedding(nn.Module):\n",
    "    def __init__(self, code_levels, code_num_in_levels, code_dims):\n",
    "        super(HierarchicalEmbedding, self).__init__()\n",
    "        self.level_num = len(code_num_in_levels)\n",
    "        self.code_levels = code_levels\n",
    "        self.level_embeddings = nn.ModuleList([nn.Embedding(code_num, code_dim) for level, (code_num, code_dim) in enumerate(zip(code_num_in_levels, code_dims))])\n",
    "\n",
    "    def forward(self, input=None):\n",
    "        embeddings = [self.level_embeddings[level](self.code_levels[:, level] - 1) for level in range(self.level_num)]\n",
    "        embeddings = torch.cat(embeddings, dim=1)\n",
    "        return embeddings#return: (code_num, embedding_size*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "P2WrkMm32VCT"
   },
   "outputs": [],
   "source": [
    "# Graph Representation + Collaborative Graph Learning\n",
    "class GraphConvBlock(torch.nn.Module):\n",
    "  def __init__(self, node_type, input_dim, output_dim, adj):\n",
    "    super(GraphConvBlock, self).__init__()\n",
    "    self.node_type = node_type\n",
    "    self.adj = adj\n",
    "    self.dense = torch.nn.Linear(input_dim, output_dim)\n",
    "    self.bn = torch.nn.BatchNorm1d(output_dim)\n",
    "    self.activation = torch.nn.ReLU()\n",
    "\n",
    "  def forward(self, embedding, embedding_neighbor, weight_decay=None):\n",
    "    output = embedding + torch.matmul(self.adj, embedding_neighbor)\n",
    "    if self.node_type == 'code':\n",
    "        assert weight_decay is not None\n",
    "        output += torch.matmul(weight_decay, embedding)\n",
    "    output = self.dense(output)\n",
    "    output = self.bn(output)\n",
    "    output = self.activation(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qrI8k2N42VCT"
   },
   "outputs": [],
   "source": [
    "class GraphLearning(torch.nn.Module):\n",
    "  def __init__(self, patient_dim, code_dim, patient_code_adj, code_code_adj, patient_hidden_dims, code_hidden_dims):\n",
    "    super(GraphLearning, self).__init__()\n",
    "    # Setup\n",
    "    self.patient_code_adj = patient_code_adj\n",
    "    self.code_patient_adj = patient_code_adj.t()\n",
    "    self.code_code_adj = code_code_adj\n",
    "    self.code_num = code_code_adj.shape[0]\n",
    "    self.sigma = torch.nn.Sigmoid()\n",
    "    # Parameters\n",
    "    # For Ontology Weights\n",
    "    self.miu = torch.nn.Parameter(torch.randn(self.code_num), requires_grad=True)\n",
    "    self.theta = torch.nn.Parameter(torch.randn(self.code_num), requires_grad=True)\n",
    "    # For L=1 Graph Layer\n",
    "    self.c2p_dense_1 = torch.nn.Linear(code_dim, patient_dim)\n",
    "    self.p2c_dense_1 = torch.nn.Linear(patient_dim, code_dim)\n",
    "    self.patient_block_1 = GraphConvBlock('patient', patient_dim, patient_hidden_dims[0], self.patient_code_adj)\n",
    "    self.code_block_1 = GraphConvBlock('code', code_dim, code_hidden_dims[0], self.code_patient_adj)\n",
    "    # For L=2 Graph Layer\n",
    "    self.p2c_dense_2 = torch.nn.Linear(patient_hidden_dims[0], code_hidden_dims[0])\n",
    "    self.code_block_2 = GraphConvBlock('code', code_hidden_dims[0], code_hidden_dims[1], self.code_patient_adj)\n",
    "\n",
    "  def forward(self, patient_embeddings, code_embeddings):\n",
    "    ontology_weight_matrix = torch.sigmoid(self.miu * self.code_code_adj + self.theta)\n",
    "    # L = 1\n",
    "    code_embeddings_p = self.c2p_dense_1(code_embeddings)\n",
    "    patient_embeddings_new = self.patient_block_1(patient_embeddings, code_embeddings_p)\n",
    "    patient_embeddings_c = self.p2c_dense_1(patient_embeddings)\n",
    "    code_embeddings = self.code_block_1(code_embeddings, patient_embeddings_c, ontology_weight_matrix)\n",
    "    patient_embeddings = patient_embeddings_new\n",
    "    # L = 2\n",
    "    patient_embeddings_c = self.p2c_dense_2(patient_embeddings)\n",
    "    code_embeddings = self.code_block_2(code_embeddings, patient_embeddings_c, ontology_weight_matrix)\n",
    "    return code_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lR4VHzWZ2VCT"
   },
   "outputs": [],
   "source": [
    "# Temporal Learning for Visits\n",
    "class VisitEmbedding(torch.nn.Module):\n",
    "  def __init__(self, max_seq_len):\n",
    "    super(VisitEmbedding, self).__init__()\n",
    "    self.max_seq_len = max_seq_len\n",
    "\n",
    "  def forward(self, code_embeddings, visit_codes, visit_lens):\n",
    "    # visit_codes: (batch_size, max_seq_len, max_code_num_in_a_visit)\n",
    "    visit_codes = visit_codes - 1\n",
    "    visit_codes_mask = (visit_codes == -1)\n",
    "    visit_codes[visit_codes_mask] = 0\n",
    "    visit_codes_mask = (1 - visit_codes_mask.float())\n",
    "    visit_codes_num = visit_codes_mask.float().sum(dim=-1).unsqueeze(-1)\n",
    "    visit_codes_embedding = code_embeddings[visit_codes] # (batch_size, max_seq_len, max_code_num_in_a_visit, code_dim)\n",
    "    visit_codes_mask = visit_codes_mask.unsqueeze(-1)\n",
    "    visit_codes_embedding *= visit_codes_mask  # (batch_size, max_seq_len, max_code_num_in_a_visit, code_dim)\n",
    "    assert not torch.isnan(visit_codes_embedding).any()\n",
    "    visit_codes_num[visit_codes_num == 0] = 1\n",
    "    visits_embeddings = torch.sum(visit_codes_embedding, dim=-2) / visit_codes_num # (batch_size, max_seq_len, code_dim)\n",
    "    assert not torch.isnan(visits_embeddings).any()\n",
    "    visit_mask = sequence_mask(visit_lens, self.max_seq_len)  # (batch_size, max_seq_len, 1)\n",
    "    visits_embeddings *= visit_mask.unsqueeze(-1)  # (batch_size, max_seq_len, code_dim)\n",
    "    return visits_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1lhXcwFI2VCU"
   },
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "  def __init__(self, input_dim, attention_dim):\n",
    "    super(Attention, self).__init__()\n",
    "    self.attention_dim = attention_dim\n",
    "    self.u_omega = torch.nn.Parameter(torch.randn(attention_dim), requires_grad=True)\n",
    "    self.w_omega = torch.nn.Parameter(torch.randn(input_dim, attention_dim), requires_grad=True)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    # x: (batch_size, max_seq_len, rnn_dim[-1] / hidden_size)\n",
    "    t = torch.matmul(x, self.w_omega)\n",
    "    vu = torch.matmul(t, self.u_omega).view(x.shape[:-1])  # (batch_size, max_seq_len)\n",
    "    vu *= mask\n",
    "    alphas = masked_softmax(vu, mask)\n",
    "    output = torch.sum(x * alphas.unsqueeze(-1), dim=-2)  # (batch_size, rnn_dim[-1] / hidden_size)\n",
    "    return output, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "L-dgDIn82VCU"
   },
   "outputs": [],
   "source": [
    "class TemporalEmbedding(torch.nn.Module):\n",
    "  def __init__(self, input_dim, rnn_dims, attention_dim, max_seq_len):\n",
    "    super(TemporalEmbedding, self).__init__()\n",
    "    self.rnn_layers = torch.nn.GRU(input_dim, rnn_dims[-1])\n",
    "    self.attention = Attention(rnn_dims[-1], attention_dim)\n",
    "    self.max_seq_len = max_seq_len\n",
    "\n",
    "  def forward(self, embeddings, lens):\n",
    "    seq_mask = sequence_mask(lens, self.max_seq_len)\n",
    "    outputs, hn = self.rnn_layers(embeddings)\n",
    "    outputs = outputs * seq_mask.unsqueeze(-1)  # (batch_size, max_seq_len, rnn_dim[-1])\n",
    "    outputs, alphas = self.attention(outputs, seq_mask)  # (batch_size, rnn_dim[-1])\n",
    "    return outputs, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "x1295XNC2VCU"
   },
   "outputs": [],
   "source": [
    "class CGL(torch.nn.Module):\n",
    "  def __init__(self, code_map, code_levels, patient_code_adj, code_code_adj, num_train_sample, max_admission_num):\n",
    "    super(CGL, self).__init__()\n",
    "    # Hierarchical Embedding for Medical Codes\n",
    "    code_num_in_levels = (np.max(code_levels, axis=0)).tolist()\n",
    "    code_levels = torch.from_numpy(code_levels).to(device)\n",
    "    code_dims = [32] * code_levels.shape[1]\n",
    "    self.hier_embed_layer = HierarchicalEmbedding(code_levels, code_num_in_levels, code_dims)\n",
    "    # Initialize Patient Embeddings\n",
    "    self.user_emb = torch.nn.Embedding(num_train_sample, 16)\n",
    "    # Collaborative Graph Learning\n",
    "    patient_code_adj = torch.from_numpy(patient_code_adj).to(device).to(torch.float32)\n",
    "    code_code_adj = torch.from_numpy(code_code_adj).to(device)\n",
    "    code_code_adj = (code_code_adj > 0).float()\n",
    "    self.graph_convolution_layer = GraphLearning(\n",
    "        patient_dim=16,\n",
    "        code_dim=sum(code_dims),\n",
    "        patient_code_adj=patient_code_adj,\n",
    "        code_code_adj=code_code_adj,\n",
    "        patient_hidden_dims=[32],\n",
    "        code_hidden_dims=[64, 128])\n",
    "    # Temporal Learning for Visits\n",
    "    self.visit_embedding_layer = VisitEmbedding(max_admission_num)\n",
    "    self.visit_temporal_embedding_layer = TemporalEmbedding(128, [200], 32, max_admission_num)\n",
    "    # Output FC Layer\n",
    "    self.output_layer = torch.nn.Linear(200, 1)\n",
    "    self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "  def forward(self, visit_codes, visit_lens):\n",
    "    # Get Hierarchical Embedding for Medical Codes\n",
    "    code_embeddings = self.hier_embed_layer(None)\n",
    "    # Get Patient Embeddings\n",
    "    patient_embeddings = self.user_emb.weight\n",
    "    # Collaborative Graph Learning\n",
    "    assert not torch.isnan(code_embeddings).any()\n",
    "    code_embeddings = self.graph_convolution_layer(patient_embeddings, code_embeddings)\n",
    "    # Temporal Learning for Visits\n",
    "    visits_embeddings = self.visit_embedding_layer(code_embeddings, visit_codes, visit_lens)\n",
    "    assert not torch.isnan(visits_embeddings).any()\n",
    "    visit_output, alpha_visit = self.visit_temporal_embedding_layer(visits_embeddings, visit_lens)\n",
    "    assert not torch.isnan(visit_output).any()\n",
    "    # Output\n",
    "    output = self.sigmoid(self.output_layer(visit_output))\n",
    "    return output, code_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njnK9pm82fQr"
   },
   "source": [
    "## **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EkPH2lhD2fQr"
   },
   "outputs": [],
   "source": [
    "model = CGL(code_map, code_levels, patient_code_adj, code_code_adj, len(train_x), torch.Tensor.size(train_x)[1])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gR_XRBd12r3k"
   },
   "outputs": [],
   "source": [
    "# Initialize evaluation record lists\n",
    "auc_list = []\n",
    "acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEFy_gNX2fQs"
   },
   "outputs": [],
   "source": [
    "# Training mode\n",
    "model.train()\n",
    "# Loss and optimizer (learning rate)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# Initialize data loader for training\n",
    "training_data = MyData(train_x, train_y, train_lens)\n",
    "train_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "total_step = len(train_loader)\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "  for i, (patients, labels, seq_lengths) in enumerate(train_loader):\n",
    "    patients = patients.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Forward pass\n",
    "    outputs, learned_code = model(patients, seq_lengths)\n",
    "    outputs = torch.reshape(outputs, (len(outputs),))\n",
    "    loss = criterion(outputs, labels.to(torch.float32))\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Tracking\n",
    "    if (i+1) % 100 == 0:\n",
    "      print('Epoch: [{}/{}], Step: [{}/{}], Loss: {}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "  if (epoch+1) % 1 == 0:\n",
    "    model.eval()\n",
    "    # Initialize data loader for testing\n",
    "    test_data = MyData(test_x, test_y, test_lens)\n",
    "    test_loader = DataLoader(test_data, batch_size=len(test_x), shuffle=True)\n",
    "    # Testing\n",
    "    for (patients, labels, seq_lengths) in test_loader:\n",
    "        visits_embeddings = model.visit_embedding_layer(learned_code, patients, seq_lengths)\n",
    "        assert not torch.isnan(visits_embeddings).any()\n",
    "        visit_output, alpha_visit = model.visit_temporal_embedding_layer(visits_embeddings, seq_lengths)\n",
    "        assert not torch.isnan(visit_output).any()\n",
    "        pred = model.sigmoid(model.output_layer(visit_output))\n",
    "        pred = torch.reshape(pred, (len(pred),))\n",
    "        pred_auc = pred.detach().cpu().numpy()\n",
    "        pred_acc = np.round(pred_auc)\n",
    "        auc_list.append(roc_auc_score(labels.detach().cpu().numpy(), pred_auc))\n",
    "        acc_list.append(accuracy_score(labels.detach().cpu().numpy(), pred_acc))\n",
    "    model.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "AK7g548ahypH",
    "dwWd0EZEh7yu",
    "5_6-6KU40vhm",
    "ou6-LZxDt9eU",
    "Mz1fEQpcdhCO"
   ],
   "machine_shape": "hm",
   "name": "CGL.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

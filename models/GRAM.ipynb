{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7RQAqYJ4PKl"
   },
   "source": [
    "# **GRAM: Graph-based Attention Model for Healthcare Representation Learning**\n",
    "Edward Choi, Mohammad Taha Bahadori, Le Song, Walter F. Stewart, Jimeng Sun\n",
    "\n",
    "[KDD 2017](https://dl.acm.org/doi/10.1145/3097983.3098126)\n",
    "\n",
    "Model Parameters in Original Paper:\n",
    "\n",
    "1.   GRU Hidden Layer Dimension: 128\n",
    "2.   Attention Layer Dimension: 128\n",
    "3.   Dropout Rate: 0.5\n",
    "4.   Code Embedding Dimension: 128\n",
    "5.   Batch Size: 100\n",
    "6.   Number of Epochs: 100\n",
    "7.   L2 Regularization Coefficient: 0.001\n",
    "8.   $x_{max}$: 100\n",
    "9.   $\\alpha$: 0.75\n",
    "\n",
    "PyTorch Implementation by [Leisheng Yu](https://github.com/ThunderbornSakana) (leisheng.yu@alumni.emory.edu)\n",
    "\n",
    "# **Diagnosis Prediction -- Multi-label Binary Prediction Task**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gl8qmBDRO34R"
   },
   "source": [
    "## **Package Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vrLL7YFROwma"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from collections import OrderedDict\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmDy5ympOI3F"
   },
   "source": [
    "## **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rFEgi0SQf2HA"
   },
   "outputs": [],
   "source": [
    "# Binary Format Combo for GRAM\n",
    "with open('../MIMIC3_data/Binary_Data_Format/binary_train_codes_x.pkl', 'rb') as f0:\n",
    "  binary_train_codes_x = pickle.load(f0)\n",
    "\n",
    "with open('../MIMIC3_data/Binary_Data_Format/binary_test_codes_x.pkl', 'rb') as f1:\n",
    "  binary_test_codes_x = pickle.load(f1)\n",
    "\n",
    "train_codes_y = np.load('../MIMIC3_data/Binary_Data_Format/train_codes_y.npy')\n",
    "train_visit_lens = np.load('../MIMIC3_data/Binary_Data_Format/train_visit_lens.npy')\n",
    "test_codes_y = np.load('../MIMIC3_data/Binary_Data_Format/test_codes_y.npy')\n",
    "test_visit_lens = np.load('../MIMIC3_data/Binary_Data_Format/test_visit_lens.npy')\n",
    "\n",
    "code_levels = np.load('../MIMIC3_data/code_related/code_levels.npy')\n",
    "patient_code_adj = np.load('../MIMIC3_data/code_related/patient_code_adj.npy')\n",
    "code_code_adj = np.load('../MIMIC3_data/code_related/code_code_adj.npy')\n",
    "\n",
    "with open('../MIMIC3_data/code_related/code_map.pkl', 'rb') as f13:\n",
    "  code_map = pickle.load(f13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y8IKHZrCOIbN"
   },
   "outputs": [],
   "source": [
    "def transform_and_pad_input(x):\n",
    "  tempX = []\n",
    "  for ele in x:\n",
    "    tempX.append(torch.tensor(ele).to(torch.float32))\n",
    "  x_padded = pad_sequence(tempX, batch_first=True, padding_value=0)\n",
    "  return x_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "amGqJWsnONLQ"
   },
   "outputs": [],
   "source": [
    "padded_X_train = transform_and_pad_input(binary_train_codes_x)\n",
    "padded_X_test = transform_and_pad_input(binary_test_codes_x)\n",
    "trans_y_train = torch.tensor(train_codes_y)\n",
    "trans_y_test = torch.tensor(test_codes_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "p_C_1hzBP35n"
   },
   "outputs": [],
   "source": [
    "class MyData(data.Dataset):\n",
    "    def __init__(self, data_seq, data_label, data_len):\n",
    "        self.data_seq = data_seq\n",
    "        self.data_label = data_label\n",
    "        self.data_len = data_len\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data_seq)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_seq[idx], self.data_label[idx], self.data_len[idx]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkYrGTJVU51l"
   },
   "source": [
    "## **Model Starts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vDfaeLx20P0"
   },
   "outputs": [],
   "source": [
    "# 2.4 Initializing Basic Embeddings -- GloVe\n",
    "# Augment all visits\n",
    "# code_num_in_levels = (np.max(code_levels, axis=0)).tolist()\n",
    "# fourth_level_num = code_num_in_levels[3]\n",
    "# third_level_num = code_num_in_levels[2]\n",
    "# second_level_num = code_num_in_levels[1]\n",
    "# augmented_list = []\n",
    "# for i in range(len(train_codes_x)):\n",
    "#   for ii in range(train_visit_lens[i]):\n",
    "#     temp_visit = train_codes_x[i][ii]\n",
    "#     temp_visit2 = temp_visit[temp_visit > 0]\n",
    "#     cleaned_visit = temp_visit2.tolist()\n",
    "#     prev_lens = len(cleaned_visit)\n",
    "#     for iii in range(prev_lens):\n",
    "#       first_idx = code_levels[cleaned_visit[iii]-1][2]\n",
    "#       second_idx = code_levels[cleaned_visit[iii]-1][1]\n",
    "#       third_idx = code_levels[cleaned_visit[iii]-1][0]\n",
    "#       cleaned_visit = cleaned_visit + [fourth_level_num+first_idx, fourth_level_num+third_level_num+second_idx, fourth_level_num+third_level_num+second_level_num+third_idx]\n",
    "#     augmented_list.append(cleaned_visit)\n",
    "\n",
    "# Creating the Co-occurrence matrix M\n",
    "# total_code_num = sum(code_num_in_levels)\n",
    "# M = np.zeros((total_code_num, total_code_num))\n",
    "# for length in range(1, 36):\n",
    "#   for width in range(length, total_code_num):\n",
    "#     for visit_count in range(len(augmented_list)):\n",
    "#       one_visit = augmented_list[visit_count]\n",
    "#       M[length][width] += one_visit.count(length+1) * one_visit.count(width+1)\n",
    "#   print(length)\n",
    "\n",
    "# Training the embedding vectors using M\n",
    "f_M = np.load(\"../MIMIC3_data/code_related/f_M.npy\")\n",
    "M_log = np.load(\"../MIMIC3_data/code_related/M_log.npy\")\n",
    "for i in range(len(M_log)):\n",
    "  for ii in range(len(M_log)):\n",
    "    if M_log[i][ii] < 0:\n",
    "      M_log[i][ii] = 0\n",
    "\n",
    "class GlovePretrain(nn.Module):\n",
    "  def __init__(self, f_M, M_log):\n",
    "    super(GlovePretrain, self).__init__()\n",
    "    self.f_M = f_M\n",
    "    self.M_log = M_log\n",
    "    self.Mlens = len(f_M)\n",
    "    self.b = torch.nn.Parameter(torch.zeros(len(f_M),))\n",
    "    self.e = torch.nn.Embedding(len(f_M), 128)\n",
    "\n",
    "  def forward(self):\n",
    "    ee = torch.matmul(self.e.weight * 1, torch.t(self.e.weight * 1))\n",
    "    b_ij = self.b.expand(self.Mlens, self.Mlens)\n",
    "    b_ij = b_ij + torch.t(b_ij)\n",
    "    J = torch.sum(f_M * torch.square(ee + b_ij - M_log))\n",
    "    return J\n",
    "\n",
    "f_M = torch.from_numpy(f_M).to(device)\n",
    "M_log = torch.from_numpy(M_log).to(device)\n",
    "\n",
    "glove = GlovePretrain(f_M, M_log)\n",
    "glove = glove.to(device)\n",
    "glove.train()\n",
    "optimizer = torch.optim.Adagrad(glove.parameters(), lr=0.05)\n",
    "num_iterations = 25000\n",
    "for iteration in range(num_iterations):\n",
    "  loss = glove()\n",
    "  # Backward and optimize\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  if iteration % 2000 == 0:\n",
    "    print(loss.item())\n",
    "pretrained_embeddings = glove.e.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dKLegSujadhN"
   },
   "outputs": [],
   "source": [
    "class GRAM(nn.Module):\n",
    "  def __init__(self, code_levels, code_num_in_levels, code_dims, hidden_dim, layer_dim, dropout_prob, output_dim, pretrained_embeddings):\n",
    "    super(GRAM, self).__init__()\n",
    "    self.level_num = len(code_num_in_levels)\n",
    "    self.code_levels = code_levels\n",
    "    self.level_embeddings = nn.ModuleList([nn.Embedding(code_num, code_dim) for level, (code_num, code_dim) in enumerate(zip(code_num_in_levels, code_dims))])\n",
    "    # Equip with the pretrained embeddings\n",
    "    self.level_embeddings[3].weight = torch.nn.Parameter(pretrained_embeddings[0:code_num_in_levels[3]])\n",
    "    self.level_embeddings[2].weight = torch.nn.Parameter(pretrained_embeddings[code_num_in_levels[3]:code_num_in_levels[3] + code_num_in_levels[2]])\n",
    "    self.level_embeddings[1].weight = torch.nn.Parameter(pretrained_embeddings[code_num_in_levels[3] + code_num_in_levels[2]:code_num_in_levels[3] + code_num_in_levels[2] + code_num_in_levels[1]])\n",
    "    self.level_embeddings[0].weight = torch.nn.Parameter(pretrained_embeddings[code_num_in_levels[3] + code_num_in_levels[2] + code_num_in_levels[1]:code_num_in_levels[3] + code_num_in_levels[2] + code_num_in_levels[1] + code_num_in_levels[0]])\n",
    "    # Attention layer\n",
    "    self.attention = nn.Linear(code_dims[3]*2, code_dims[3])\n",
    "    self.u_a = nn.Linear(code_dims[3], 1, bias=False)\n",
    "    # GRU layers for processing sequences\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.layer_dim = layer_dim\n",
    "    self.gru = nn.GRU(code_dims[3], hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob)\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    self.softmax0 = torch.nn.Softmax(dim=1)\n",
    "    self.softmax = torch.nn.Softmax()\n",
    "    self.tanh = torch.nn.Tanh()\n",
    "\n",
    "  def forward(self, x, x_len):\n",
    "    # Initializing hidden state for first input with zeros\n",
    "    weight0 = next(self.parameters()).data\n",
    "    h0 = weight0.new(self.layer_dim, x.size(0), self.hidden_dim).zero_().to(device)\n",
    "    h0 = h0.data\n",
    "    # Update the code embeddings\n",
    "    embeddings = [self.level_embeddings[level](self.code_levels[:, level] - 1) for level in range(self.level_num)]\n",
    "    score_matrix = self.softmax0(torch.concat((self.u_a(self.attention(torch.concat((embeddings[3], embeddings[0]), dim=1))), self.u_a(self.attention(torch.concat((embeddings[3], embeddings[1]), dim=1))), self.u_a(self.attention(torch.concat((embeddings[3], embeddings[2]), dim=1))), self.u_a(self.attention(torch.concat((embeddings[3], embeddings[3]), dim=1)))), dim=1))\n",
    "    new_emb_matrix = embeddings[0] * torch.reshape(score_matrix[:, 0], (len(score_matrix[:, 0]), 1)) + embeddings[1] * torch.reshape(score_matrix[:, 1], (len(score_matrix[:, 1]), 1)) + embeddings[2] * torch.reshape(score_matrix[:, 2], (len(score_matrix[:, 2]), 1)) + embeddings[3] * torch.reshape(score_matrix[:, 3], (len(score_matrix[:, 3]), 1))\n",
    "    # Get visit embeddings\n",
    "    x = self.tanh(torch.matmul(x, new_emb_matrix))\n",
    "    # Feed into GRU\n",
    "    x_packed = pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
    "    # Forward propagation by passing in the input and hidden state into the model\n",
    "    out, _ = self.gru(x_packed, h0)\n",
    "    out, out_lengths = pad_packed_sequence(out, batch_first=True)\n",
    "    # Reshaping the outputs in the shape of (batch_size, hidden_size)\n",
    "    # so that it can fit into the fully connected layer\n",
    "    out = out[list(torch.arange(len(out)).cpu()), list((out_lengths-1).cpu()), :]\n",
    "    # Last layer with softmax\n",
    "    out = self.softmax(self.fc(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sS7eqiFV4wV"
   },
   "source": [
    "## **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jJusLhWfQ5f3"
   },
   "outputs": [],
   "source": [
    "code_num_in_levels = (np.max(code_levels, axis=0)).tolist()\n",
    "code_levels2 = torch.from_numpy(code_levels).to(device)\n",
    "model = GRAM(code_levels2, code_num_in_levels, [128, 128, 128, 128], 128, 2, 0.5, 4880, pretrained_embeddings)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "b2BthMo_UFyW"
   },
   "outputs": [],
   "source": [
    "# This is for diagnosis prediction\n",
    "def evaluate_model(pred, label, k1, k2, k3, k4, k5, k6):\n",
    "  pred2_k1 = torch.zeros_like(pred[0])\n",
    "  pred3_k1 = []\n",
    "  pred2_k2 = torch.zeros_like(pred[0])\n",
    "  pred3_k2 = []\n",
    "  pred2_k3 = torch.zeros_like(pred[0])\n",
    "  pred3_k3 = []\n",
    "  pred2_k4 = torch.zeros_like(pred[0])\n",
    "  pred3_k4 = []\n",
    "  pred2_k5 = torch.zeros_like(pred[0])\n",
    "  pred3_k5 = []\n",
    "  pred2_k6 = torch.zeros_like(pred[0])\n",
    "  pred3_k6 = []\n",
    "  # above is for recall and precision\n",
    "  true3 = [] # this is for label\n",
    "  pred4 = [] # this is for ndcg\n",
    "  for i in range(len(pred)):\n",
    "    pred2_k1[torch.topk(pred[i], k1).indices] = 1\n",
    "    pred3_k1.append(pred2_k1.cpu().detach().tolist())\n",
    "    pred2_k2[torch.topk(pred[i], k2).indices] = 1\n",
    "    pred3_k2.append(pred2_k2.cpu().detach().tolist())\n",
    "    pred2_k3[torch.topk(pred[i], k3).indices] = 1\n",
    "    pred3_k3.append(pred2_k3.cpu().detach().tolist())\n",
    "    pred2_k4[torch.topk(pred[i], k4).indices] = 1\n",
    "    pred3_k4.append(pred2_k4.cpu().detach().tolist())\n",
    "    pred2_k5[torch.topk(pred[i], k5).indices] = 1\n",
    "    pred3_k5.append(pred2_k5.cpu().detach().tolist())\n",
    "    pred2_k6[torch.topk(pred[i], k6).indices] = 1\n",
    "    pred3_k6.append(pred2_k6.cpu().detach().tolist())\n",
    "    pred4.append(pred[i].cpu().detach().tolist())\n",
    "    true3.append(label[i].cpu().detach().tolist())\n",
    "  \n",
    "  metric_p_1 = precision_score(true3, pred3_k1, average='samples')\n",
    "  metric_p_2 = precision_score(true3, pred3_k2, average='samples')\n",
    "  metric_p_3 = precision_score(true3, pred3_k3, average='samples')\n",
    "  metric_p_4 = precision_score(true3, pred3_k4, average='samples')\n",
    "  metric_p_5 = precision_score(true3, pred3_k5, average='samples')\n",
    "  metric_p_6 = precision_score(true3, pred3_k6, average='samples')\n",
    "  \n",
    "  metric_r_1 = recall_score(true3, pred3_k1, average='samples')\n",
    "  metric_r_2 = recall_score(true3, pred3_k2, average='samples')\n",
    "  metric_r_3 = recall_score(true3, pred3_k3, average='samples')\n",
    "  metric_r_4 = recall_score(true3, pred3_k4, average='samples')\n",
    "  metric_r_5 = recall_score(true3, pred3_k5, average='samples')\n",
    "  metric_r_6 = recall_score(true3, pred3_k6, average='samples')\n",
    "  \n",
    "  metric_n_1 = ndcg_score(true3, pred4, k=k1)\n",
    "  metric_n_2 = ndcg_score(true3, pred4, k=k2)\n",
    "  metric_n_3 = ndcg_score(true3, pred4, k=k3)\n",
    "  metric_n_4 = ndcg_score(true3, pred4, k=k4)\n",
    "  metric_n_5 = ndcg_score(true3, pred4, k=k5)\n",
    "  metric_n_6 = ndcg_score(true3, pred4, k=k6)\n",
    "  return metric_p_1, metric_r_1, metric_n_1, metric_p_2, metric_r_2, metric_n_2, metric_p_3, metric_r_3, metric_n_3, metric_p_4, metric_r_4, metric_n_4, metric_p_5, metric_r_5, metric_n_5, metric_p_6, metric_r_6, metric_n_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3pXA4xAHUG01"
   },
   "outputs": [],
   "source": [
    "# Initialize evaluation record lists\n",
    "metric_p1_list = []\n",
    "metric_p2_list = []\n",
    "metric_p3_list = []\n",
    "metric_p4_list = []\n",
    "metric_p5_list = []\n",
    "metric_p6_list = []\n",
    "metric_r1_list = []\n",
    "metric_r2_list = []\n",
    "metric_r3_list = []\n",
    "metric_r4_list = []\n",
    "metric_r5_list = []\n",
    "metric_r6_list = []\n",
    "metric_n1_list = []\n",
    "metric_n2_list = []\n",
    "metric_n3_list = []\n",
    "metric_n4_list = []\n",
    "metric_n5_list = []\n",
    "metric_n6_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4OPwVinUS8v"
   },
   "outputs": [],
   "source": [
    "# Training mode\n",
    "model.train()\n",
    "# Loss and optimizer (learning rate)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.001)\n",
    "# Initialize data loader for training\n",
    "training_data = MyData(padded_X_train, trans_y_train, train_visit_lens)\n",
    "train_loader = DataLoader(training_data, batch_size=100, shuffle=True)\n",
    "total_step = len(train_loader)\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "  for i, (patients, labels, seq_lengths) in enumerate(train_loader):\n",
    "    patients = patients.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Forward pass\n",
    "    outputs = model(patients, seq_lengths)\n",
    "    loss = criterion(outputs, labels.to(torch.float32))\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Tracking\n",
    "    if (i+1) % 20 == 0:\n",
    "      print('Epoch: [{}/{}], Step: [{}/{}], Loss: {}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "  if (epoch+1) % 1 == 0:\n",
    "    model.eval()\n",
    "    # Initialize data loader for testing\n",
    "    test_data = MyData(padded_X_test, trans_y_test, test_visit_lens)\n",
    "    test_loader = DataLoader(test_data, batch_size=len(padded_X_test), shuffle=True)\n",
    "    # Testing\n",
    "    for (patients, labels, seq_lengths) in test_loader:\n",
    "        patients = patients.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = model(patients, seq_lengths)\n",
    "        # Subject to Change! @k for evaluation\n",
    "        metric_p1, metric_r1, metric_n1, metric_p2, metric_r2, metric_n2, metric_p3, metric_r3, metric_n3, metric_p4, metric_r4, metric_n4, metric_p5, metric_r5, metric_n5, metric_p6, metric_r6, metric_n6, = evaluate_model(pred, labels, 5, 10, 15, 20, 25, 30)\n",
    "        ###############################\n",
    "        metric_p1_list.append(metric_p1)\n",
    "        metric_p2_list.append(metric_p2)\n",
    "        metric_p3_list.append(metric_p3)\n",
    "        metric_p4_list.append(metric_p4)\n",
    "        metric_p5_list.append(metric_p5)\n",
    "        metric_p6_list.append(metric_p6)\n",
    "        ###############################\n",
    "        metric_r1_list.append(metric_r1)\n",
    "        metric_r2_list.append(metric_r2)\n",
    "        metric_r3_list.append(metric_r3)\n",
    "        metric_r4_list.append(metric_r4)\n",
    "        metric_r5_list.append(metric_r5)\n",
    "        metric_r6_list.append(metric_r6)\n",
    "        ###############################\n",
    "        metric_n1_list.append(metric_n1)\n",
    "        metric_n2_list.append(metric_n2)\n",
    "        metric_n3_list.append(metric_n3)\n",
    "        metric_n4_list.append(metric_n4)\n",
    "        metric_n5_list.append(metric_n5)\n",
    "        metric_n6_list.append(metric_n6)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wi2p1MkH67eM"
   },
   "source": [
    "# **Mortality Prediction -- Binary Prediction Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlmcf7DgiEJx"
   },
   "source": [
    "## **Package Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A0K5WfT7682J"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from collections import OrderedDict\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lo-MsS82iM1o"
   },
   "source": [
    "## **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "z4ywBWdyiPqn"
   },
   "outputs": [],
   "source": [
    "# Binary Format Combo for GRAM\n",
    "with open('../MIMIC3_data/Binary_Data_Format/binary_train_codes_x.pkl', 'rb') as f0:\n",
    "  binary_train_codes_x = pickle.load(f0)\n",
    "\n",
    "with open('../MIMIC3_data/Binary_Data_Format/binary_test_codes_x.pkl', 'rb') as f1:\n",
    "  binary_test_codes_x = pickle.load(f1)\n",
    "\n",
    "train_visit_lens = np.load('../MIMIC3_data/Binary_Data_Format/train_visit_lens.npy')\n",
    "train_mort = np.load('../MIMIC3_data/Binary_Data_Format/train_mort.npy')\n",
    "test_visit_lens = np.load('../MIMIC3_data/Binary_Data_Format/test_visit_lens.npy')\n",
    "test_mort = np.load('../MIMIC3_data/Binary_Data_Format/test_mort.npy')\n",
    "\n",
    "code_levels = np.load('../MIMIC3_data/code_related/code_levels.npy')\n",
    "patient_code_adj = np.load('../MIMIC3_data/code_related/patient_code_adj.npy')\n",
    "code_code_adj = np.load('../MIMIC3_data/code_related/code_code_adj.npy')\n",
    "\n",
    "with open('../MIMIC3_data/code_related/code_map.pkl', 'rb') as f13:\n",
    "  code_map = pickle.load(f13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "F4z0EPmyiarL"
   },
   "outputs": [],
   "source": [
    "def transform_and_pad_input(x):\n",
    "  tempX = []\n",
    "  for ele in x:\n",
    "    tempX.append(torch.tensor(ele).to(torch.float32))\n",
    "  x_padded = pad_sequence(tempX, batch_first=True, padding_value=0)\n",
    "  return x_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oZesRh28ieo1"
   },
   "outputs": [],
   "source": [
    "padded_X_train = transform_and_pad_input(binary_train_codes_x)\n",
    "padded_X_test = transform_and_pad_input(binary_test_codes_x)\n",
    "trans_y_train = torch.tensor(train_mort)\n",
    "trans_y_test = torch.tensor(test_mort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3rsnYX7FikM-"
   },
   "outputs": [],
   "source": [
    "class MyData(data.Dataset):\n",
    "    def __init__(self, data_seq, data_label, data_len):\n",
    "        self.data_seq = data_seq\n",
    "        self.data_label = data_label\n",
    "        self.data_len = data_len\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data_seq)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_seq[idx], self.data_label[idx], self.data_len[idx]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-trzBpBy7Ry"
   },
   "source": [
    "## **Model Starts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCcYaAnMy-k4"
   },
   "outputs": [],
   "source": [
    "# 2.4 Initializing Basic Embeddings -- GloVe\n",
    "# Augment all visits\n",
    "# code_num_in_levels = (np.max(code_levels, axis=0)).tolist()\n",
    "# fourth_level_num = code_num_in_levels[3]\n",
    "# third_level_num = code_num_in_levels[2]\n",
    "# second_level_num = code_num_in_levels[1]\n",
    "# augmented_list = []\n",
    "# for i in range(len(train_codes_x)):\n",
    "#   for ii in range(train_visit_lens[i]):\n",
    "#     temp_visit = train_codes_x[i][ii]\n",
    "#     temp_visit2 = temp_visit[temp_visit > 0]\n",
    "#     cleaned_visit = temp_visit2.tolist()\n",
    "#     prev_lens = len(cleaned_visit)\n",
    "#     for iii in range(prev_lens):\n",
    "#       first_idx = code_levels[cleaned_visit[iii]-1][2]\n",
    "#       second_idx = code_levels[cleaned_visit[iii]-1][1]\n",
    "#       third_idx = code_levels[cleaned_visit[iii]-1][0]\n",
    "#       cleaned_visit = cleaned_visit + [fourth_level_num+first_idx, fourth_level_num+third_level_num+second_idx, fourth_level_num+third_level_num+second_level_num+third_idx]\n",
    "#     augmented_list.append(cleaned_visit)\n",
    "\n",
    "# Creating the Co-occurrence matrix M\n",
    "# total_code_num = sum(code_num_in_levels)\n",
    "# M = np.zeros((total_code_num, total_code_num))\n",
    "# for length in range(1, 36):\n",
    "#   for width in range(length, total_code_num):\n",
    "#     for visit_count in range(len(augmented_list)):\n",
    "#       one_visit = augmented_list[visit_count]\n",
    "#       M[length][width] += one_visit.count(length+1) * one_visit.count(width+1)\n",
    "#   print(length)\n",
    "\n",
    "# Training the embedding vectors using M\n",
    "f_M = np.load(\"../MIMIC3_data/code_related/f_M.npy\")\n",
    "M_log = np.load(\"../MIMIC3_data/code_related/M_log.npy\")\n",
    "for i in range(len(M_log)):\n",
    "  for ii in range(len(M_log)):\n",
    "    if M_log[i][ii] < 0:\n",
    "      M_log[i][ii] = 0\n",
    "\n",
    "class GlovePretrain(nn.Module):\n",
    "  def __init__(self, f_M, M_log):\n",
    "    super(GlovePretrain, self).__init__()\n",
    "    self.f_M = f_M\n",
    "    self.M_log = M_log\n",
    "    self.Mlens = len(f_M)\n",
    "    self.b = torch.nn.Parameter(torch.zeros(len(f_M),))\n",
    "    self.e = torch.nn.Embedding(len(f_M), 128)\n",
    "\n",
    "  def forward(self):\n",
    "    ee = torch.matmul(self.e.weight * 1, torch.t(self.e.weight * 1))\n",
    "    b_ij = self.b.expand(self.Mlens, self.Mlens)\n",
    "    b_ij = b_ij + torch.t(b_ij)\n",
    "    J = torch.sum(f_M * torch.square(ee + b_ij - M_log))\n",
    "    return J\n",
    "\n",
    "f_M = torch.from_numpy(f_M).to(device)\n",
    "M_log = torch.from_numpy(M_log).to(device)\n",
    "\n",
    "glove = GlovePretrain(f_M, M_log)\n",
    "glove = glove.to(device)\n",
    "glove.train()\n",
    "optimizer = torch.optim.Adagrad(glove.parameters(), lr=0.05)\n",
    "num_iterations = 25000\n",
    "for iteration in range(num_iterations):\n",
    "  loss = glove()\n",
    "  # Backward and optimize\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  if iteration % 2000 == 0:\n",
    "    print(loss.item())\n",
    "pretrained_embeddings = glove.e.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0VOWIgt5y_TW"
   },
   "outputs": [],
   "source": [
    "class GRAM(nn.Module):\n",
    "  def __init__(self, code_levels, code_num_in_levels, code_dims, hidden_dim, layer_dim, dropout_prob, output_dim):\n",
    "    super(GRAM, self).__init__()\n",
    "    self.level_num = len(code_num_in_levels)\n",
    "    self.code_levels = code_levels\n",
    "    self.level_embeddings = nn.ModuleList([nn.Embedding(code_num, code_dim) for level, (code_num, code_dim) in enumerate(zip(code_num_in_levels, code_dims))])\n",
    "    # Equip with the pretrained embeddings\n",
    "    self.level_embeddings[3].weight = torch.nn.Parameter(pretrained_embeddings[0:code_num_in_levels[3]])\n",
    "    self.level_embeddings[2].weight = torch.nn.Parameter(pretrained_embeddings[code_num_in_levels[3]:code_num_in_levels[3] + code_num_in_levels[2]])\n",
    "    self.level_embeddings[1].weight = torch.nn.Parameter(pretrained_embeddings[code_num_in_levels[3] + code_num_in_levels[2]:code_num_in_levels[3] + code_num_in_levels[2] + code_num_in_levels[1]])\n",
    "    self.level_embeddings[0].weight = torch.nn.Parameter(pretrained_embeddings[code_num_in_levels[3] + code_num_in_levels[2] + code_num_in_levels[1]:code_num_in_levels[3] + code_num_in_levels[2] + code_num_in_levels[1] + code_num_in_levels[0]])\n",
    "    # Attention layer\n",
    "    self.attention = nn.Linear(code_dims[3]*2, code_dims[3])\n",
    "    self.u_a = nn.Linear(code_dims[3], 1, bias=False)\n",
    "    # GRU layers for processing sequences\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.layer_dim = layer_dim\n",
    "    self.gru = nn.GRU(code_dims[3], hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob)\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    self.softmax0 = torch.nn.Softmax(dim=1)\n",
    "    self.sigmoid = torch.nn.Sigmoid()\n",
    "    self.tanh = torch.nn.Tanh()\n",
    "\n",
    "  def forward(self, x, x_len):\n",
    "    # Initializing hidden state for first input with zeros\n",
    "    weight0 = next(self.parameters()).data\n",
    "    h0 = weight0.new(self.layer_dim, x.size(0), self.hidden_dim).zero_().to(device)\n",
    "    h0 = h0.data\n",
    "    # Update the code embeddings\n",
    "    embeddings = [self.level_embeddings[level](self.code_levels[:, level] - 1) for level in range(self.level_num)]\n",
    "    score_matrix = self.softmax0(torch.concat((self.u_a(self.attention(torch.concat((embeddings[3], embeddings[0]), dim=1))), self.u_a(self.attention(torch.concat((embeddings[3], embeddings[1]), dim=1))), self.u_a(self.attention(torch.concat((embeddings[3], embeddings[2]), dim=1))), self.u_a(self.attention(torch.concat((embeddings[3], embeddings[3]), dim=1)))), dim=1))\n",
    "    new_emb_matrix = embeddings[0] * torch.reshape(score_matrix[:, 0], (len(score_matrix[:, 0]), 1)) + embeddings[1] * torch.reshape(score_matrix[:, 1], (len(score_matrix[:, 1]), 1)) + embeddings[2] * torch.reshape(score_matrix[:, 2], (len(score_matrix[:, 2]), 1)) + embeddings[3] * torch.reshape(score_matrix[:, 3], (len(score_matrix[:, 3]), 1))\n",
    "    # Get visit embeddings\n",
    "    x = self.tanh(torch.matmul(x, new_emb_matrix))\n",
    "    # Feed into GRU\n",
    "    x_packed = pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
    "    # Forward propagation by passing in the input and hidden state into the model\n",
    "    out, _ = self.gru(x_packed, h0)\n",
    "    out, out_lengths = pad_packed_sequence(out, batch_first=True)\n",
    "    # Reshaping the outputs in the shape of (batch_size, hidden_size)\n",
    "    # so that it can fit into the fully connected layer\n",
    "    out = out[list(torch.arange(len(out)).cpu()), list((out_lengths-1).cpu()), :]\n",
    "    # Last layer with sigmoid\n",
    "    out = self.sigmoid(self.fc(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlHqAK390v41"
   },
   "source": [
    "## **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pV0GbkzXzLN_"
   },
   "outputs": [],
   "source": [
    "code_num_in_levels = (np.max(code_levels, axis=0)).tolist()\n",
    "code_levels2 = torch.from_numpy(code_levels).to(device)\n",
    "model = GRAM(code_levels2, code_num_in_levels, [128, 128, 128, 128], 128, 2, 0.5, 1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qC9AAV8LzOm5"
   },
   "outputs": [],
   "source": [
    "# Initialize evaluation record lists\n",
    "auc_list = []\n",
    "acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n07WZtDLzZVw"
   },
   "outputs": [],
   "source": [
    "# Training mode\n",
    "model.train()\n",
    "# Loss and optimizer (learning rate)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.001)\n",
    "# Initialize data loader for training\n",
    "training_data = MyData(padded_X_train, trans_y_train, train_visit_lens)\n",
    "train_loader = DataLoader(training_data, batch_size=len(padded_X_train), shuffle=True)\n",
    "total_step = len(train_loader)\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "  for i, (patients, labels, seq_lengths) in enumerate(train_loader):\n",
    "    patients = patients.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Forward pass\n",
    "    outputs = model(patients, seq_lengths)\n",
    "    outputs = torch.reshape(outputs, (len(outputs),))\n",
    "    loss = criterion(outputs, labels.to(torch.float32))\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Tracking\n",
    "    if (i+1) % 1 == 0:\n",
    "      print('Epoch: [{}/{}], Step: [{}/{}], Loss: {}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "  if (epoch+1) % 1 == 0:\n",
    "    model.eval()\n",
    "    # Initialize data loader for testing\n",
    "    test_data = MyData(padded_X_test, trans_y_test, test_visit_lens)\n",
    "    test_loader = DataLoader(test_data, batch_size=len(padded_X_test), shuffle=True)\n",
    "    # Testing\n",
    "    for (patients, labels, seq_lengths) in test_loader:\n",
    "        patients = patients.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = model(patients, seq_lengths)\n",
    "        pred = torch.reshape(pred, (len(pred),))\n",
    "        pred_auc = pred.detach().cpu().numpy()\n",
    "        pred_acc = np.round(pred_auc)\n",
    "        auc_list.append(roc_auc_score(labels.detach().cpu().numpy(), pred_auc))\n",
    "        acc_list.append(accuracy_score(labels.detach().cpu().numpy(), pred_acc))\n",
    "    model.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "-7RQAqYJ4PKl",
    "gl8qmBDRO34R",
    "EmDy5ympOI3F",
    "XkYrGTJVU51l",
    "-sS7eqiFV4wV",
    "tlmcf7DgiEJx",
    "Lo-MsS82iM1o",
    "xlHqAK390v41"
   ],
   "machine_shape": "hm",
   "name": "GRAM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
